Panoramica sul framework Apache Spark.

---

# Hadoop
**Hadoop** è un framework open-source progettato per l'elaborazione distribuita di grandi volumi di dati su cluster di computer. Il suo cuore è **MapReduce**, un modello di programmazione che divide i compiti in fasi di `map` e `reduce`, e utilizza **HDFS (Hadoop Distributed File System)** per immagazzinare i dati.

Punti chiave:
- Lavora principalmente su **disco**: ogni fase scrive e legge su HDFS, il che può essere lento.
- È molto robusto e tollerante ai guasti.
- Ottimo per _batch processing_ pesante e strutturato.

Con l'aumentare della popolarità di Hadoop si è cominciato ad usarlo per applicazioni più complesse e spesso iterative (e.g. iterative machine learning).

Hadoop usa in modo massiccio il disco fisso e quindi manca di primitive efficienti per la condivisione dei dati; di conseguenza questo rende **le applicazioni sopracitate troppo lente e macchinose**, perché ad ogni iterazione i dati vengono prima letti dal disco fisso e poi scritti $\rightarrow$ uso intendo di I/O.

---
# Spark
**Spark è una naturale evoluzione di Hadoop**, pensato per casi d’uso moderni che richiedono più flessibilità e velocità.

**Apache Spark** nasce proprio per superare i limiti di Hadoop. Anch'esso è un framework distribuito, ma invece di appoggiarsi solo sul disco, **usa massicciamente la memoria RAM** tramite una struttura chiamata **RDD (Resilient Distributed Dataset)**.

Vantaggi principali:
- **10 - 100 volte più veloce** di Hadoop per molte operazioni.
- Ideale per **applicazioni iterative** (es. machine learning) e **query interattive**.
- Offre molte **API semplici e una pipeline unificata** per trasformazioni e azioni sui dati.
- I dati vengono letti e scritti sul disco fisso solamente una volta rispetto a tutte le iterazioni di una singola applicazione.

Obiettivi di Spark:
- costruire un'**astrazione della memoria di lavoro distribuita** che sia **tollerante ai guasti ed efficiente**
- costruire un **sistema unificato che include librerie utili** per risolvere la maggior parte dei problemi

## RDD (Resilient Distributed Datasets)
Un **RDD** è una **collezione distribuita di oggetti**, memorizzata in RAM su più nodi del cluster. È progettato per:
- essere **fault-tolerant** (cioè capace di recuperare da errori senza perdere i dati)
- **gestire dati in RAM** per accelerare i calcoli
- supportare **operazioni parallele**

Nella pratica, un RDD può:
- puntare direttamente a una sorgente dati (come HDFS o un database)
- derivare da **trasformazioni** (e.g. `map` e `reduce` ma anche `filter`, `join`, eccetera) a partire da altri RDD

### Tipi di operazioni e dipendenze
Un programma Spark è composto da una serie di operazioni sugli RDD.
Gli RDD supportano **due tipi principali di operazioni**:
1. **Transformations** (ritornano nuovi RDD):
	- insieme di operazioni su un RDD che definiscono come deve essere trasformato
	- Sono **lazy**: non eseguono nulla finché non si invoca un’action, questo permette un'ottimizzazione pre-esecuzione
    - Esempi: `map`, `filter`, `flatMap`, `reduceByKey`, `join`
2. **Actions** (eseguono calcoli veri e propri):
	- Attivano la catena di trasformazioni per restituire un risultato
	- Alcune azioni memorizzano i dati direttamente sul disco fisso, altre li passano ai driver necessari
    - Esempi: `count`, `collect`, `first`, `saveAsTextFile`


Con gli RDD invece di salvare ogni dato trasformato su disco, Spark tiene traccia di _come_ un RDD è stato generato, **tramite dipendenze**. Se un nodo fallisce, può ricostruire i dati partendo dall'origine.
- **Narrow dependencies**: ogni partizione figlia dipende da una sola partizione genitore (e.g. `map` e `filter`)
- **Wide dependencies**: una partizione figlia dipende da più partizioni genitore (e.g. `reduceByKey`)

## Componenti di Spark
Di seguito viene riportata graficamente l'architettura di Apache Spark:

![[spark-image-1| 100%]]

- **Driver Program** - l'applicazione principale scritta dall'analista nel linguaggio preferito, essa contiene:
	- **Spark Context** - oggetto chiave usato per collegarsi alle funzionalità di Spark, una volta creato:
		- comunica con il Cluster Manager
		- coordina la creazione degli RDD, grazie al RDD Directed Acyclic Graph
		- pianifica l'esecuzione delle Task
- **Cluster Manager** - componente esterno che gestisce le risorse hardware (i nodi) del cluster. Spark può appoggiarsi a diversi cluster managers, come per esempio Apache YARN o Kubernetes
- **Executors** - processi lanciati sui vari nodi del cluster. Eseguono le task assegnate e gestiscono in memoria i dati in cache. Essi si interfacciano direttamente con il driver per riportare i risultati
	- **Task** - unità minima di lavoro in Spark, un insieme di task compongono un lavoro

