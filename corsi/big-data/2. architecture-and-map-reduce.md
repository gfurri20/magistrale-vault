Introduzione agli elementi architetturali principali dei sistemi distribuiti e definizione del paradigma Map-Reduce.

---

# Architectural Elements
Gli elementi principali che compongono un elaboratore sono tre:
- CPU e GPU (i.e. processor unit)
- Memoria di lavoro (e.g. RAM), veloce ma limitata
- Memoria per lo storage, lenta ma capiente

Unità di processo e memoria di lavoro riescono ad eseguire algoritmi semplici, all'interno dei quali il carico di dati è limitato (e.g. calcoli statistici).
Se si aggiunge la possibilità di usare memoria solida, allora è possibile computare un maggior numero di dati caricando in memoria dei chunk dell'intero dataset (e.g. data mining).
## Sistemi distribuiti
Per ovviare alla quantità di memoria solida limitata del singolo elaboratore sono nati i **Data Centers**: un gruppo di server (detto cluster) orchestrati tramite algoritmi appositi e comunicanti tramite la rete.
Ogni server è chiamato nodo, un'insieme di nodi (tra 16 e 64) è detto rack.

**Problemi principali** di un sistema distribuito:
- **decomposizione delle task** da risolvere in unità elaborabili parallelamente
- **parallelizzazione** delle unità da elaborare
- **sincronizzazione e schedulazione delle task**, l'assegnamento ha come obiettivi:
	- la velocizzazione dell'esecuzione
	- sfruttamento di ogni risorsa
	- riduzione dell'impatto in caso di fallimento
- **distribuzione dello storage** nei vari nodi
	- introduzione della ridondanza (e aumento conseguente della complessità)
- **collo di bottiglia sulla rete**, nel caso di spostamento di grandi quantità di dati
	- diventa utile spostare la computazione dove sono i dati e non il contrario

Come i computer normali, **anche i data center si appoggiano ad un sistema operativo** fondamentale per la gestione delle risorse e dei servizi del sistema.

## Soluzione designata
Per gestire al meglio un sistema distribuito è necessaria un'architettura specifica (e.g. Hadoop e **Spark** sono modelli il cui scopo è quello di gestire operazioni di analisi dati su sistemi distribuiti).

![[architecture-and-map-reduce-image-1.excalidraw | 100%]]

Ogni sistema distribuito è caratterizzato da elementi architetturali che permettono:
- il controllo centralizzato
- l'elaborazione distribuita

### Controllo centralizzato
Il controllo centralizzato è gestito da due elementi:
- **central storage orchestrator** - gestisce la distribuzione dei file dell'intero sistema
- **central job orchestrator** - coordina la schedulazione delle task

### Elaborazione distribuita
L'elaborazione distribuita è gestita da due elementi installati in ogni nodo del sistema:
- **local storage daemon** - gestisce la memorizzazione fisica dei dati distribuiti
- **local job execution daemon** - eseguono fisicamente i task impartiti dal controllo centralizzato

## Workflow
Comunemente quando si intende sfruttare la potenza di un data center per eseguire dei calcoli si eseguono, prima i test, in una simulazione su macchina locale:
1. il dataset viene campionato ed importato in una macchina locale
2. l'algoritmo viene ideato, implementato e testato sul campione del dataset su una macchina locale
3. una volta che l'algoritmo soddisfa l'analista allora esso viene applicato sul dataset intero all'interno del datacenter
4. analisi dei risultati

---

# MapReduce
MapReduce è un modello di programmazione basato sui concetti di programmazione funzionale e nel particolare sulle funzioni `map` e `reduce`, che operano sui dataset:
- `map` - applica una funzione a tutti gli elementi di un dataset, ottenendo un **risultato intermedi**o (può essere fatto in parallelo)
- `reduce` - applica funzioni di aggregazione sui dati (e.g. somma, media, ...), ottenendo un **risultato finale**
Fornisce, in questo modo, **un modo semplice per scrivere programmi che possono essere automaticamente suddivisi e distribuiti su più macchine**, rendendo possibili elaborazioni su dataset enormi.

Quindi un framework d'esecuzione in grado di:
- lavorare su processi di dati a larga scala
- lavorare in appositi data centers

MapReduce è una combinazione di **paradigma di programmazione** e **sistema di esecuzione distribuito**, usato per analizzare enormi quantità di dati in modo scalabile, efficiente e resistente ai guasti.

## Data Structure
La struttura base di processi MapReduce sono le **coppie chiave-valore**.
$$(\texttt{key}, \texttt{value})$$
Ogni valore del dataset viene trasformato in coppie chiave-valore che poi sono utilizzate in elaborazione, esse possono essere combinati in vari e complessi modi per la creazione di diversi algoritmi.

## Workflow
1. il dataset intero viene distribuito a blocchi tra tutti nodi elaboratori
2. il mapper genera il risultato intermedio applicando una trasformazione ad ogni istanza chiave-valore
3. il reducer applica un'aggregazione su ogni coppia avente stessa chiave, ottenendo un risultato finale

Implicitamente, tra la fase di map e quella di reduce c'è un **raggruppamento distribuito** sulle chiavi intermedie.

Dopo la riduzione, le chiavi di output verranno scritte sul filesystem distribuito, in questo modo si avranno tanti file di output quanti numero di reducer.
I risultati, infine, potranno essere usati per essere presentati o per essere consumati in un'altra elaborazione, creando una pipeline di elaborazione.

I dati intermedi non sono salvati sul filesystem.